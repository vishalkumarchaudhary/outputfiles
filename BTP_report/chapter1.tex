\chapter{Introduction}
\pagenumbering{arabic}\hspace{3mm}
\section{Motivation}
Board games are one of the best way to test agents and reinforcement learning algorithms and  its viability.These are complex yet not too complex than that of agent in real environments. Example- chess is one of the complex game with state space of almost $10^{47}$ and decision tree of size $10^{123}$. Similarly state space complexity of connect4 is $10^{12}$ and decision tree of $10^{21}$. This motivates me to learn and contribute to algorithms in board games. The task of this project is to improve the learning rate of the algorithm. The traditional supervised machine learning method involves experts data and sometimes, it is not enough or the data is too noisy such that we cannot learn anything. In this project various games have been chosen depending upon its complexity to train agent within time constraints.

\section{Road of the project}
The project started with establishing fixing the baseline for some fixed game. We went directly to study the one of the most complex game chess. We thought to train agent with the help of data set. But training such network and work on big data has certain limitations. Like it requires huge computation power (GPU and RAM requirements) and there should be enough examples for a certain strategy to learn. This poses a problem that we don't know the quality of data though we had collected a good amount of data. So that strategy didn't worked. 


We got to know about learning through self-play. We started trying to select a game that could suit our situation like low computation, trainable in days. We select 2048 game which has the branching factor of $4$ which means at any given position we can have maximum of $4$ valid moves. The motivation of choosing low branching factor game was to test the possibility and scope of given algorithm. We tried and trained the 2048 and it was able to train in 2-3 hours with better results. We tried with different type of neural network architecture. We found out that the irrespective of the network architecture, there was not significant change.
So we move on applying the algorithm without changing the neural network architecture and we want to test the algorithm on some complex games with higher branching factor. We selected connect4 game with branching factor of $7$. We tested and studied the algorithm of self-play and every piece of it on connect4. We established the baseline over this game and we will not change the neural network architecture in any experiments because of findings in the game 2048.

\subsection{Connect4 game and confidence bounds}
Connect4 is an example of sequential game in which player1 moves after the player2 has taken its action. So the task of agent is to take action which maximize its probability of winning. The advantage of training agent from self play is that it explores the game state space all by itself depending upon the reward it gets after taking those action.To make the probability space of more rewarding state higher, better bound should be chosen to select the action. We gave a shot to confidence intervals, that are solutions of multi-arm bandit problem. KL-UCB \cite{klucb} is found to be performing better than the Upper Confidence Bound (UCB1). We applied this bound in the training process and got better results which excited us to seek for more better bound in the multi-arm bandit field. We found Thompson sampling and experimented on it. We found out that it is performing as better as the KL-UCB. In the end our both results are being better than result of baseline which uses UCB1.

\subsection{Simultaneous games}
With the given achievement in sequential games, we also wanted to try on simultaneous games.
Simultaneous games are the game in which both player make their move at the same time. So there is randomness involved and the agent have to choose action considering the possible move made by adversary. There is only a very little work on this area. We have taken cricket game in simultaneous environment. So we have bowler team and they chooses which bowler to bowl for a over. We have also batting team which chooses shot which can be $0, 1, 2, 4, 6$ runs irrespective of the knowledge of the bowler. This shot from the batting team is fixed for the whole over for the simplicity of game. The motive of bowling network is to minimize the number of runs made instead number of matches won whereas the motive of batting network to maximize score. Since we want to investigate the learning through self-play in simultaneous game which is much harder for agent to learn than the sequential games, we made this cricket environment to be simple and easy to investigate. 
